\[Simon Tatham, 2025-02-14\]

-   [Introduction](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#intro)
-   [XOR in boolean logic](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#boolean)
    -   [What is it?](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#what)
        -   [â€œExclusive ORâ€](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#exclusive-or)
        -   [The â€˜not equalsâ€™ operator](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#unequal)
        -   [Conditional inversion](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#condinv)
        -   [Parity, or sum mod 2](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#mod2)
        -   [Difference mod 2](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#mod2diff)
    -   [What properties does it have?](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#properties)
-   [Bitwise XOR on integers](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#bitwise)
    -   [Still has all the same properties](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#properties-bitwise)
    -   [Bitwise difference](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#diff-bitwise)
    -   [Bitwise conditional inverter](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#condinv-bitwise)
    -   [Addition or subtraction without carrying](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#carryless)
-   [Some applications of XOR](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#applications)
    -   [Cryptography: combine a plaintext with a keystream](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#cryptography)
    -   [Pixel graphics: draw things so itâ€™s easy to undraw them again](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#pixels)
    -   [The â€œhalf-adder identityâ€](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#halfadder)
        -   [Averaging two integers](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#average)
        -   [Implementing XOR itself](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#constructing-xor)
    -   [Swapping bits in a word](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#bitswap)
        -   [Using XOR and shifts](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#shift)
        -   [Using XOR and just a two-bit word](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#parity)
    -   [Three XORs make a swap](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#swap)
    -   [Winning at the game of Nim](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#nim)
-   [Mathematical concepts that look like XOR](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#maths)
    -   [Symmetric difference of sets](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#symmdiff)
    -   [Groups of exponent 2](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#exp2)
    -   [Nim-sum](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#nimsum)
    -   [_GF_(2), the finite field of order 2](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#gf2)
        -   [Linear algebra over _GF_(2)](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#gf2linalg)
            -   [Error-correcting codes](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#ecc)
        -   [Polynomials over _GF_(2)](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#gf2poly)
            -   [CRCs](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#crc)
            -   [Making larger finite fields](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#fields)
-   [Footnotes](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnotes)

## Introduction

Recently I was called on to explain the â€˜XORâ€™ operator to somebody who vaguely knew of its existence, but didnâ€™t have a good intuition for what it was useful for and how it behaved.

For me, this was one of those â€˜future shockâ€™ moments when you realise the world has moved on. When I got started in computers, you had to do low-level bit twiddling to get anything very interesting done, so you pretty much couldnâ€™t _avoid_ learning about XOR. But these days, to a high-level programmer, itâ€™s much more of an optional thing, and you can perfectly well not know much about it.

So I collected some thoughts together and gave a lecture on XOR. Slightly to my own surprise, I was able to spend a full hour talking about it â€“ and then over the course of the next couple of weeks I remembered several other things I could usefully have mentioned.

And once Iâ€™d gone to the effort of collecting all those thoughts into one place, it seemed a waste not to write it all down somewhere more permanent. (The collecting is the hardest part!)

So hereâ€™s a text version of my XOR talk â€“ or rather, the talk that it _would_ have been if Iâ€™d remembered to include everything the first time round.

## XOR in boolean logic

Weâ€™ll start by looking at XOR as a boolean logic operator, or equivalently a logic gate: a function that takes two individual bits, and outputs a single bit.

### What is it?

If Iâ€™m going right back to first principles, then I should start by actually _defining_ XOR.

Any boolean function of two inputs can be defined by showing its truth table: for each of the four combinations of inputs, say what the output is. So Iâ€™ll start by showing the truth table for XOR.

| _a_ | _b_ | _a_Â XORÂ _b_ |
| --- | --- | --- |
| 0 | 0 | 0 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 0 |

<table><tbody><tr><td colspan="2" rowspan="2"></td><th colspan="2"><i>b</i></th></tr><tr><th>0</th><th>1</th></tr><tr><th rowspan="2"><i>a</i></th><th>0</th><td>0</td><td>1</td></tr><tr><th>1</th><td>1</td><td>0</td></tr></tbody></table>

The truth table of XOR, shown in two different ways

But just saying what it _is_ doesnâ€™t give a good intuition for what things itâ€™s useful for, or when to use it. So in the next few sections Iâ€™ll present a few different ways of _thinking about_ what XOR does.

For such a simple operation, itâ€™s possible to describe what it does in a lot of quite different-looking ways. But all of them are true at once! So you donâ€™t need to _keep_ thinking about XOR as being just one of the following concepts. You can suddenly switch from thinking of it one way to another, any time thatâ€™s convenient. Nothing stops being true if you do.

#### â€œExclusive ORâ€

The X in â€˜XORâ€™ stands for the word â€œexclusiveâ€. (Some assembly language syntaxes abbreviate it as â€˜EORâ€™ instead, on the grounds that â€œexclusiveâ€ doesnâ€™t actually begin with an X. But _most_ people prefer the X spelling, in my experience.)

In the normal English language, the conjunction â€˜orâ€™ has an ambiguity: if I say â€˜you can do this _or_ thatâ€™, and in fact someone tries to do both this _and_ that, does that count? It depends on the context. A parent telling a child â€œYou can have this dessert _or_ that oneâ€ certainly means â€˜but not bothâ€™ â€“ thatâ€™s the whole point. But on the other hand, if the same parent wants the child to do something useful, and says â€œDo your homework, or tidy your room!â€, theyâ€™d probably be extra _pleased_ if the child did both.

So, when you want to be clear about which version of â€˜orâ€™ you mean, you might say that youâ€™re _including_ the â€˜bothâ€™ case, or _excluding_ it.

In computing, â€˜ORâ€™ as a boolean operator is always taken to mean the _inclusive_ version: _A_ or _B_ or both. And when you want _A_ or _B_ but _not_ both, you talk about _exclusive_ OR.

Looking at the truth tables above, you can see that thatâ€™s exactly what the XOR operation is doing:

-   If _a_Â =Â 1, _or_ if _b_Â =Â 1, then _a_Â XORÂ _b_Â =Â 1.
-   But not both: if _a_ _and_ _b_ are both 1, then _a_Â XORÂ _b_Â =Â 0.

#### The â€˜not equalsâ€™ operator

Another way to look at the same truth table is: _a_Â XORÂ _b_Â =Â 1 whenever the inputs _a_ and _b_ are _different_ from each other. If theyâ€™re the same (either both 0 or both 1), then _a_Â XORÂ _b_Â =Â 0.

So you could look at _a_Â XORÂ _b_ as meaning the same thing as _a_Â â‰ Â _b_: itâ€™s a way to _compare_ two boolean values with each other.

#### Conditional inversion

A third way to look at the same truth table is to consider each value of one of the inputs _a_, and look at what XOR does to the other variable _b_ in each case:

-   If _a_Â =Â 0, then _a_Â XORÂ _b_ is the same thing as just _b_.
-   If _a_Â =Â 1, then _a_Â XORÂ _b_ is the _opposite_ of _b_: 0 becomes 1 and 1 becomes 0.

So another way to look at the XOR operation is that youâ€™re either going to leave _b_ alone, or invert it (swapping 0 and 1), and _a_ tells you which one to do. If you imagine XOR as a tiny computing device, you could think of the input _b_ as â€˜dataâ€™ that the device is operating on, and _a_ as a â€˜controlâ€™ input that tells the device what to do with the data, with the possible choices being â€˜flipâ€™ or â€˜donâ€™t flipâ€™.

_a_Â XORÂ _b_ means: **invert _b_, but only if _a_ is true.**

But the same is _also_ true the other way round, because _a_Â XORÂ _b_ is the same thing as _b_Â XORÂ _a_. You can swap your point of view to thinking of _a_ as the â€˜dataâ€™ input and _b_ as â€˜controlâ€™, and nothing changes â€“ the operation is the same either way round.

That is, _a_Â XORÂ _b_ also means: **invert _a_, but only if _b_ is true!**

#### Parity, or sum mod 2

Hereâ€™s yet _another_ way to look at the XOR operation. _a_Â XORÂ _b_ tells you whether an _odd number_ of the inputs are true:

-   If _a_Â =Â _b_Â =Â 0, then _zero_ of the inputs are true. 0 is even, and _a_Â XORÂ _b_Â =Â 0.
-   If _a_Â =Â 1 and _b_Â =Â 0, or the other way round, then _one_ of the inputs is true. 1 is odd, and _a_Â XORÂ _b_Â =Â 1.
-   If _a_Â =Â _b_Â =Â 1, then _two_ of the inputs are true. 2 is even, and _a_Â XORÂ _b_Â =Â 0 again.

Put another way: if you _add together_ the two inputs, and then reduce the result modulo 2 (that is, divide by 2 and take the remainder), you get the same answer as _a_Â XORÂ _b_.

In particular, if you XOR together _more_ than two values, the overall result will tell you whether an odd or even number of the inputs were 1 â€“ no matter how many inputs you combine.

#### Difference mod 2

So XOR corresponds to addition mod 2. But it also corresponds to _subtraction_ mod 2, at the same time: if you take the difference _a_Â âˆ’Â _b_ and reduce _that_ mod 2, you still get the same answer _a_Â XORÂ _b_!

-   If _a_Â =Â _b_, then the difference _a_Â âˆ’Â _b_ is 0, and so is _a_Â XORÂ _b_.
-   If _a_Â â‰ Â _b_, then the difference _a_Â âˆ’Â _b_ is either +1 or âˆ’1. Mod 2, those are the same as each other â€“ theyâ€™re both just 1.

### What properties does it have?

If you have a complicated boolean expression involving lots of XORs, itâ€™s useful to know how you can manipulate the expression to simplify it.

To begin with, XOR is both **commutative** and **associative**, which mean (respectively) that

_a_Â XORÂ _b_Â =Â _b_Â XORÂ _a_

(_a_Â XORÂ _b_)Â XORÂ _c_Â =Â _a_Â XORÂ (_b_Â XORÂ _c_)

In practice, what this means is that if you see a long list of values or variables XORed together, you _donâ€™t need to worry_ about what order theyâ€™re combined in, because it doesnâ€™t make any difference. You can rearrange the list of inputs into any order you like, and choose any subset of them to combine first, and the answer will be the same regardless.

(This is perhaps most easily seen by thinking of XOR as â€˜addition mod 2â€™, because addition is also both commutative and associative â€“ whether or not itâ€™s mod anything â€“ so XOR inherits both properties.)

Secondly, **0 acts as an identity**, which means that XORing anything with zero just gives you the same thing you started with:

_a_Â XORÂ 0Â =Â 0Â XORÂ _a_Â =Â _a_

So if you have a long list of things XORed together, and you can see that one of them is 0, you can just delete it from the list.

Thirdly, **everything is self-inverse**<sup id="footnote-inverse">1</sup>I apologise for using â€˜invertâ€™ and â€˜inverseâ€™ in two senses in this article. In boolean logic, â€˜invertingâ€™ a signal generally means the NOT operator, interchanging 1 with 0 (or true with false, if you prefer). But in mathematics, an â€˜inverseâ€™ is a value that cancels out another value to get you back to the identity, in a way which varies depending on the operation you use to combine them (if youâ€™re talking about addition itâ€™s âˆ’_x_, and for multiplication itâ€™s 1/_x_). I hope that everywhere Iâ€™ve used the word at all itâ€™s clear from context which sense I mean it in.1: if you XOR any value with _itself_, you always get zero.

_a_Â XORÂ _a_Â =Â 0

One consequence of this is that if you have a long list of variables being XORed together, and the same variable occurs twice in the list, you can just remove both copies. _Anything appearing twice cancels out._ For example, the two copies of _b_ can be removed in this expression:

_a_Â XORÂ _b_Â XORÂ _c_Â XORÂ _b_Â =Â _a_Â XORÂ _c_

Another consequence is that if you have a value _a_Â XORÂ _b_, and you want to recover just _a_, you can do it by XORing in another copy of _b_ (if you know it), to cancel the one thatâ€™s already in the expression. _Putting something in a second time is the same as removing it:_

(_a_Â XORÂ _b_)Â XORÂ _b_Â =Â _a_Â XORÂ (_b_Â XORÂ _b_)Â =Â _a_Â XORÂ 0Â =Â _a_

## Bitwise XOR on integers

Now weâ€™ve seen what XOR does on individual bits, itâ€™s time to apply it to something larger.

To a computer itâ€™s most natural to represent an integer in binary, so that it looks like a string of 0s and 1s. So if you have two integers, you can combine them _bitwise_ (that is, â€˜treating each bit independentlyâ€™) using a logical operation like XOR: you write the numbers in binary one above the other<sup id="footnote-twos-complement">2</sup>What if the integers are negative? Normally, in the finite integer sizes that computers handle in hardware, negative integers are represented in twoâ€™s complement, i.e. mod 2<sup><i>n</i></sup>. So âˆ’1 is the integer with all bits 1, for example. Thereâ€™s a reasonably natural way to extend this to _arbitrarily large_ integers, by pretending the string of 1 bits on the left goes all the way to infinity (and it can even be made mathematically rigorous!). In this view, the XOR of two positive numbers, or two negative numbers, is positive, because at a high enough bit position each bit is XORing two 0s or two 1s; but the XOR of a positive and negative number is negative, because sooner or later youâ€™re always XORing a 0 with a 1 bit. Some languages, such as Python, actually implement this â€“ you can try it out at the interactive prompt!2, and set each output bit to the result of combining the corresponding bits of both input numbers via XOR.

Here are a couple of examples, one small and carefully chosen, and the other large and random-looking:

<table><tbody><tr><td></td><th>Binary</th><th>Hex</th><th>Decimal</th></tr><tr><th><i>a</i></th><td><code>1010</code></td><td><code>A</code></td><td><code>10</code></td></tr><tr><th><i>b</i></th><td><code>1100</code></td><td><code>C</code></td><td><code>12</code></td></tr><tr><th><i>a</i>&nbsp;XOR&nbsp;<i>b</i></th><td><code>0110</code></td><td><code>6</code></td><td><code>&nbsp;6</code></td></tr></tbody></table>

<table><tbody><tr><td></td><th>Binary</th><th>Hex</th><th>Decimal</th></tr><tr><th><i>a</i></th><td><code>11001111001000011111000101111011</code></td><td><code>CF21F17B</code></td><td><code>3475108219</code></td></tr><tr><th><i>b</i></th><td><code>10011011010000100100111001011101</code></td><td><code>9B424E5D</code></td><td><code>2604813917</code></td></tr><tr><th><i>a</i>&nbsp;XOR&nbsp;<i>b</i></th><td><code>01010100011000111011111100100110</code></td><td><code>5463BF26</code></td><td><code>1415823142</code></td></tr></tbody></table>

Small and large examples of applying bitwise XOR to two integers

The small example contains one bit for each element of the XOR truth table. If you look at vertically aligned bits in the binary column of the table, youâ€™ll see that in the rightmost place both _a_ and _b_ have a 0 bit; in the leftmost place they both have a 1 bit; in between, thereâ€™s a position where only _a_ has a 1, and one where only _b_ has a 1. And in each position, the output bit is the boolean XOR of the two input bits.

Of course, this idea of â€˜bitwiseâ€™ logical operations â€“ taking an operation that accepts a small number of input bits, and applying it one bit at a time to a whole binary integer â€“ is not limited to XOR. Bitwise AND and bitwise OR are also well defined operations, and both useful. But this particular article is about XOR, so Iâ€™ll stick to that one.

### Still has all the same properties

[Earlier](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#properties) I discussed a number of mathematical laws obeyed by the one-bit version of XOR: itâ€™s commutative, associative, has 0 as an identity, and every possible input is self-inverse in the sense that XORing it with itself gives 0.

In the bitwise version applied to integers, all of these things are still true:

-   _a_Â XORÂ _b_Â =Â _b_Â XORÂ _a_ for any integers _a_ and _b_
-   (_a_Â XORÂ _b_)Â XORÂ _c_Â =Â _a_Â XORÂ (_b_Â XORÂ _c_) similarly
-   _a_Â XORÂ 0Â =Â 0Â XORÂ _a_Â =Â _a_, where 0 means the _integer_ zero, i.e. with all its binary bits 0
-   _a_Â XORÂ _a_Â =Â 0

So if you have a complicated expression containing bitwise XORs, you can simplify it in all the same ways you could do it with single-bit XORs.

### Bitwise difference

With individual bits, I [said earlier](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#unequal) that _a_Â XORÂ _b_ means the same thing as _a_Â â‰ Â _b_: given two input bits, it returns 1 if the bits are different from each other, and 0 if theyâ€™re the same.

Of course, applied bitwise to whole integers, this is true separately in every bit position: each bit of the output integer tells you whether the two corresponding input bits were unequal.

So if _a_Â =Â _b_, then _a_Â XORÂ _b_Â =Â 0. And if _a_Â â‰ Â _b_, then _a_Â XORÂ _b_Â â‰ Â 0, because two unequal integers must have disagreed in at least one bit position, so that bit will be set in their XOR.

So in some sense you can still use bitwise XOR to tell you whether two entire integers are equal: itâ€™s 0 if they are, and nonzero if theyâ€™re not. But bitwise XOR gives you more detail than that: it also gives you a specific list of _which bit positions_ they differ in.

### Bitwise conditional inverter

I also said [earlier](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#condinv) that you could see XOR as a â€˜conditional inversionâ€™ operator: imagine one input _b_ to be â€˜dataâ€™, and the other input _a_ to be a â€˜controlâ€™ input that says whether you want to flip the data bit.

Applied bitwise to whole integers, this is still true, but more usefully so: the control input _a_ says _which_ data bits you want to flip.

For example, in the Unicode character encoding (and also in its ancestor ASCII), every character you might need to store in a string is represented by an integer. The choice of integer encodings has some logic to it (at least in places). In particular, the upper-case Latin letters A,Â B,Â C,Â â€¦,Â Z and their lower-case equivalents a,Â b,Â c,Â â€¦,Â z have encodings that differ in just one bit: A is 65, or binary 01000001, and a is 97, or binary 01100001. So if you know that a character value represents a Latin letter<sup id="footnote-emoji">3</sup>Of course, this doesnâ€™t apply to _all_ Unicode characters! Most donâ€™t have a concept of upper or lower case at all. And unfortunately, this rule isnâ€™t even obeyed by all of the characters that do. It was consistently true in ASCII, and in some of the descendants of ASCII, but Unicode as a whole wasnâ€™t able to stick 100% to the principle. If you take this too far, you might get strange ideas, like the lower-case version of the car emoji being a â€˜no pedestriansâ€™ sign: \>>> chr(ord('ğŸš—') ^ 32)  
'ğŸš·'3, you can swap it from upper case to lower case _or vice versa_ by flipping just that one bit between 0 and 1.

And you can do that most easily, in machine code or any other programming language, by XORing it with a number that has just that one bit set, namely binary 00100000, or decimal 32. For example, in Python (where the `^` operator means bitwise XOR):

```
&gt;&gt;&gt; chr(ord('A') ^ 32)
'a'
&gt;&gt;&gt; chr(ord('x') ^ 32)
'X'
```

Flipping case by XORing with 32 in Python

Of course, the other thing I said earlier also applies: it doesnâ€™t matter _which_ of the inputs to XOR you regard as the data, and which as the control. The operation is the same both ways round.

In particular, suppose youâ€™ve already XORed two values _a_ and _b_ to obtain a number _d_ that tells you which bits they differ in. Then you can turn either of _a_ or _b_ into the other one, by XORing with the difference _d_, because that flips exactly the set of bits where _a_ has the opposite value to _b_.

In other words, these three statements are all _equivalent_ â€“ if any one of them is true, then so are the other two:

_a_Â XORÂ _b_Â =Â _d_  
_a_Â XORÂ _d_Â =Â _b_  
_b_Â XORÂ _d_Â =Â _a_

### Addition or subtraction without carrying

Another thing I mentioned about XOR on single bits is that it corresponds to [addition mod 2](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#mod2), or equivalently, [subtraction mod 2](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#mod2diff).

That isnâ€™t quite how it works once you do it bitwise on whole words<sup id="footnote-well-actually">4</sup>I suppose, _technically_, you could argue that it is still literally true that _a_Â XORÂ _b_, _a_Â +Â _b_, and _a_Â âˆ’Â _b_ are all congruent to each other mod 2, because all that means is that they all have the same low-order bit, which they do. But that isnâ€™t a particularly _useful_ thing to say, because it ignores the way all the higher-order bits do something completely different!4. Each individual pair of bits is added mod 2, but each of those additions is independent.

One way to look at this is: suppose you were a schoolchild just learning addition, and youâ€™d simply forgotten that it was necessary to carry an overflowed sum between digit positions at all. So for every column of the sum, youâ€™d add up the input digits in that column, write down the low-order digit of whatever you got, and ignore the carry completely.

If this imaginary schoolchild were to do this procedure in binary rather than decimal, the operation theyâ€™d be performing is exactly bitwise XOR! **Bitwise XOR is like binary addition, without any carrying.**

[Later on](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#halfadder) Iâ€™ll show an interesting consequence of this, by considering what _does_ happen to the carry bits, and how you can put them back in again afterwards. Thereâ€™s also a [mathematically meaningful and useful interpretation](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#gf2poly) of the corresponding â€˜carrylessâ€™ version of _multiplication_, in which you make a shifted version of one of the inputs for each 1 bit in the other, and then add them together in this carryless XOR style instead of via normal addition.

## Some applications of XOR

Now weâ€™ve seen what XOR _is_, and a few different things itâ€™s _like_, letâ€™s look at some things itâ€™s used for.

### Cryptography: combine a plaintext with a keystream

XOR is used all over cryptography, in many different ways. I wonâ€™t go into the details of _all_ of those ways â€“ Iâ€™m sure I donâ€™t even know all of them myself! â€“ but Iâ€™ll show a simple and commonly used one.

Suppose you have a message to encrypt. One really simple thing you could do would be to take an equally long stream of random-looking data â€“ usually known as a â€˜keystreamâ€™ â€“ and combine the two streams, a byte or a word at a time, in some way that the receiver can undo. So if your message was â€œhelloâ€, for example, you might simply transmit a byte made by combining â€œhâ€ with keystream byte #1, then one thatâ€™s â€œeâ€ combined with keystream byte #2, and so on.

This seems absurdly simple â€“ surely real cryptography is terrifyingly complicated compared to this? But it really is a thing that happens. As long as each byte is combined with the keystream byte in a way that makes it possible to recover the original byte at the other end, this is a completely sensible way to encrypt a message<sup id="footnote-integrity">5</sup>However, encrypting the message is only half the story. Given a good-quality keystream, this technique is good enough to assure _confidentiality_, meaning that an eavesdropper canâ€™t find out what youâ€™re saying. But it doesnâ€™t do one single thing to ensure _integrity_, meaning that if your message is modified by an attacker, the receiver can detect the tampering and know not to trust the modified message. Integrity protection is a completely separate problem. A common mistake in novice cryptosystem design is to leave it out, assuming that if nobody can figure out what the message says, then â€œsurelyâ€ nobody can work out how to tamper with it in a useful way either. Even with more complicated encryption methods than what Iâ€™m describing here, this never goes well!5!

Thatâ€™s not to say that there isnâ€™t terrifyingly complicated stuff _somewhere_ in the setup. But in this particular context, the complicated part is in how you _generate_ your stream of random-looking bytes; the final step where you combine it with the message is the easy part. One option is for your keystream to be a huge amount of genuinely random data, as big as the total size of all messages youâ€™ll ever need to send; this is known as a â€˜one-time padâ€™, and is famously actually unbreakable â€“ but also amazingly impractical for almost all purposes. More usually you use a stream cipher, or a block cipher run in a counter mode, to expand a manageably small actual _key_ into a keystream as long as you need.

Anyway. Iâ€™ve so far left out the detail of exactly how you â€œcombineâ€ the keystream with the message. But given the subject of this article, you can probably guess that itâ€™s going to be XOR.

XOR isnâ€™t the only thing that would _work_. If your message and your keystream are each made of bytes, then there are plenty of other ways to combine two bytes that let you recover one of them later. For example, addition mod 2<sup>8</sup> would be fine: you could make each encrypted byte by _adding_ the message byte and keystream byte, and then the receiver (who has access to exactly the same keystream) would subtract the keystream byte from the sum to recover the message byte.

But in practice, XOR is generally whatâ€™s used, because itâ€™s simpler. Not so much for software â€“ CPUs generally have an â€˜ADDâ€™ instruction and an â€˜XORâ€™ instruction which are just as fast and easy to use as each other â€“ but encryption is also often done in hardware, by specially made circuitry. And if youâ€™re building hardware, addition is more complicated than XOR, because you have to worry about carrying between bit positions, which costs more space on the chip (extra logic gates) and also extra time (propagating signals from one end to the other of the addition). XOR avoids both problems: in custom hardware, itâ€™s far cheaper.

(Itâ€™s also _very_ slightly more convenient that the sender and receiver donâ€™t have to do _different_ operations. With XOR, the sender who applies the keystream and the receiver who takes it off again are both doing exactly the same thing, instead of one adding and the other subtracting.)

### Pixel graphics: draw things so itâ€™s easy to undraw them again

Letâ€™s set the wayback machine to the mid-1980s, and go back in time to when computers were smaller and simpler (at least, the kind you could afford to have in your home). Home computer graphics systems stored a very small number of bits for each pixel on the screen, meaning that you could only display a limited number of different colours at a time; and even with that limitation on framebuffer size, home computers had so little RAM in total that it was a struggle to store two entire copies of what was displayed on the screen and still have enough memory for anything else (like whatever program was _generating_ that screenful of graphics).

In an environment limited like that, what do you do if you want to draw an object that appears and disappears, or that moves around gradually?

If your moving object is _opaque_, then every time you â€˜undrawâ€™ it, you have to restore whatever was supposed to be on the screen behind it. That means you have to _remember_ what was behind it â€“ either by storing the actual pixels, or by storing some recipe that knows how to recreate the missing patch of graphics from scratch. Either one costs memory, and the second option probably costs time as well, and you donâ€™t have a lot of either to spare.

Nevertheless, you do that when you really have to. But when you _donâ€™t_ have to, itâ€™s always helpful to take shortcuts.

So one common graphical dodge was: _donâ€™t make graphical objects opaque if you donâ€™t have to_. Any time you can get away with it, prefer to draw a thing on the screen in a way that is _reversible_: combine each pixel _M_ of the moving object with the existing screen pixel _S_, in such a way that you can undo the operation later, recovering the original screen pixel _S_ from the combined value _C_ by remembering what _M_ was.

As in the previous section, there are plenty of ways to do this in principle. You could imagine treating the bits of each pixel as an _n_\-bit integer for some small _n_, and doing addition and subtraction on them (again, mod 2<sup><i>n</i></sup>). For example, you could draw by addition, setting _C_Â =Â _S_Â +Â _M_, and undraw by subtraction to recover _S_Â =Â _C_Â âˆ’Â _M_.

But these systems typically had far fewer than 8 bits per pixel, so each byte of the screen would have more than one pixel packed into it. Or, worse, the screen might be laid out in â€˜bit planesâ€™, with several widely separated blocks of memory each containing one bit of every pixel. Doing ordinary addition on the pixel values is very awkward in both cases.

In particular, consider the first of those possibilities, where you have several pixels packed into a byte. Suppose the 8 bits of the byte are treated as two 4-bit integers, or four 2-bit integers. In order to do parallel _addition_ of each of those small chunks, you canâ€™t use the normal CPUâ€™s addition instruction, because a carry off the top of one pixel value propagates into the next, causing unwanted graphical effects. So youâ€™d somehow need to arrange to suppress the carry _between_ pixels, but leave the carry _within_ a pixel alone.

So itâ€™s much easier not to try this in the first place. Combining the pixel values via XOR instead of addition means that you automatically avoid carries between pixels, because there are no carries _at all_. This is also more convenient in the â€˜bit planeâ€™ memory layout, because each bit of a pixel is treated independently; if you tried to do ordinary addition in that situation, youâ€™d have to make extra effort to propagate carries between corresponding bits in each bit plane.

Hereâ€™s a simple example, in which two lines have been drawn crossing each other. You can see that in the second diagram, drawn using XOR, thereâ€™s a missing pixel at the point where the two lines cross. That pixel is part of _both_ lines, so itâ€™s been drawn twice. Each time it was drawn, the pixel value flipped between black and white, so drawing it twice sets it back to the background colour.

That missing pixel looks like a tiny blemish. The first version of the picture looks _nicer_. But it makes it harder to undraw one of those lines later. If you just undraw it by setting all the pixels of one line back to white, then you leave a hole in the remaining line. And if you donâ€™t want to leave a hole, then you need some method of remembering which pixel _not_ to reset to white.

In the XOR version of the picture, itâ€™s the easiest thing in the world to undraw one line and leave the other one undamaged. Simply draw the line _again_ that you want to undraw. _Putting a second copy of something into an XOR combination is the same as removing it._ This kind of graphical â€˜blemishâ€™ was considered a reasonable price to pay, in situations where efficient undrawing and redrawing was needed.

Most types of 1980s home computers had an option to draw in XOR mode, for this kind of reason. It was one of the easiest ways for a beginning computer programmer to draw pretty animations. A common type of demo was to simulate two points moving around the screen along a pair of independent paths, and draw a line between the two points at each moment â€“ but to allow the previous few versions of the line to persist as well as the latest one, undrawing each one after it had been on the screen for a few frames.

Hereâ€™s an example of the kind of thing. In this example each end of the line is following a path given by a different [Lissajous curve](https://en.wikipedia.org/wiki/Lissajous_curve):

![](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/animated-lines.gif)

Moving-line animation using XOR drawing

Because weâ€™re drawing the lines in XOR mode, the procedure for turning each frame of this animation into the next involves drawing only two lines: the new one that weâ€™re adding at the leading edge of the motion, and the one thatâ€™s about to vanish from the trailing edge. So the program only has to remember the endpoint coordinates of the 10 (or however many) lines it currently has on screen â€“ or perhaps not even bother with that, and just recalculate the (_n_Â âˆ’Â 10)th coordinate in each frame to work out what line to undraw.

So this lets you produce pretty and interesting results, with almost no memory cost (you donâ€™t have to store extra copies of lots of pixels). It uses very little CPU as well (you donâ€™t have to redraw _all_ the lines in every frame, only the ones youâ€™re adding and removing), which makes the animation able to run faster.

This style of animation was a common thing for beginning programmers to play with, but it was also used by professionals. Perhaps most famously, a moving line drawn in this style acted as the main antagonist of the 1981 video game [Qix](https://en.wikipedia.org/wiki/Qix).

Even in the 1990s, with slightly larger computers, XOR-based drawing and undrawing was still a useful thing: in early GUI environments like the Amiga, when you dragged a window around the screen with the mouse, the system didnâ€™t have enough CPU to redraw the entire window as you went. Instead it would draw an outline frame for where the window was going to end up, and once youâ€™d put it in the right place, youâ€™d let go of the mouse button and the window would be properly redrawn in the new location, just once. And again the outline frame would be drawn using XOR, so that it didnâ€™t cost extra memory to remember how to undraw it on each mouse movement.

But going back to the above animation: if you watch it carefully, you might notice that when the moving endpoints change direction and a lot of the lines cross each other, the missing pixels at the crossing points give rise to a sort of interference effect. Those missing pixels started off as a necessity to save memory, but it turns out theyâ€™re also quite pretty in their own right.

Another favourite kind of 1980s graphical program for beginners was to explore those interference patterns more fully. Hereâ€™s an image created by drawing a sequence of lines, all with one endpoint in the bottom left corner of the frame, and with the other endpoints being every single pixel along the top row. In ordinary drawing mode this would just be a slow way to fill in a big black triangle. But in XOR mode, the lines interfere with each other wherever more than one of them passes through the same pixel:

![](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/interfering-lines.png)

Interference pattern between overlapping lines drawn with XOR

Thatâ€™s actually interesting, and it takes some time to work out whatâ€™s going on!

The topmost row is all black, because every pixel on that row is drawn just once, by the line whose endpoint is at exactly that position. But half way up, thereâ€™s an all-_white_ row, because the _n_ lines in the image have only _n_Â /Â 2 pixels to pass through on their way to the top row. So you get two lines passing through each pixel, so every pixel is drawn exactly twice, and cancels out back to white. Similarly, a third of the way up thereâ€™s another black row where every pixel is drawn exactly _three_ times, and so on. And generally, 1Â /Â _n_ of the way from bottom to top, you expect an all-black row if _n_ is odd, or an all-white row if _n_ is even.

But in between those 1Â /Â _n_ rows, something much more interesting happens, when the number of lines going through each pixel isnâ€™t the same everywhere on the row. Those interference patterns are related to the distribution of rounding errors in computing the horizontal position of each pixel of the line.

Iâ€™ll end this section with a question that _I_ donâ€™t know the answer to. Suppose you continue this interference-pattern drawing further to the right, past the point where the line slopes at 45Â°:

![](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/interfering-lines-2.png)

A similar interference pattern going beyond the 45Â° slope

At the 45Â° mark, something changes fundamentally. Those horizontal black and white stripes have changed direction, and now theyâ€™re sloping at what looks like a constant angle.

I understand why _something_ has changed. In this style of pixelated line drawing, a line thatâ€™s closer to horizontal than vertical can have multiple pixels on each row, and only one in each column, whereas if itâ€™s closer to vertical than horizontal, itâ€™s the other way round. So thatâ€™s a good reason why the picture should look fundamentally different in _some_ way on opposite sides of the 45Â° line.

But why does that give rise to a _consistently sloping_ set of coloured stripes, and not any other kind of weird effect? Iâ€™ve never sat down and worked that one out.

### The â€œhalf-adder identityâ€

I said [in a previous section](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#mod2) that one way to look at the expression _a_Â XORÂ _b_ is that itâ€™s the remainder mod 2 of _a_Â +Â _b_. Another way to say that is that itâ€™s the _low-order binary digit_ of _a_Â +Â _b_.

What about the high one?

To answer that, letâ€™s start with a truth table:

| _a_ | _b_ | _a_Â +Â _b_ | High bit | Low bit |
| --- | --- | --- | --- | --- |
| 0 | 0 | 0 | 0 | 0 |
| 0 | 1 | 1 | 0 | 1 |
| 1 | 0 | 1 | 0 | 1 |
| 1 | 1 | 2 | 1 | 0 |

Truth tables for the two bits of the sum _a_Â +Â _b_

Here, Iâ€™ve shown the actual value of the sum _a_Â +Â _b_ for each possible input. Then Iâ€™ve written each of those values in binary, so that 0 and 1 become 00 and 01, and 2 becomes 10. You can see that the high bit of the sum is only 1 if _both_ inputs are set.

In other words: the low bit of _a_Â +Â _b_ is _a_Â XORÂ _b_, and the high bit is _a_Â ANDÂ _b_. In other words, _a_Â +Â _b_Â =Â (_a_Â XORÂ _b_)Â +Â 2Â Ã—Â (_a_Â ANDÂ _b_).

That makes sense, if _a_ and _b_ are individual bits. What if we did the same operations bitwise, on whole integers?

If _a_ and _b_ are 32-bit integers (for example), then weâ€™ve already [seen above](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#carryless) that _a_Â XORÂ _b_ is the integer you get if you add the bits in each position, but not carrying from one bit position to the next. But also, _a_Â ANDÂ _b_ is an integer consisting of _exactly_ those carry bits: the _n_th bit of _a_Â ANDÂ _b_ is 1 precisely when a carry bit _should_ have been generated in that bit position but wasnâ€™t.

If there should have been a carry bit out of the _n_th place, then it should have been added to the (_n_Â +Â 1)st place. But itâ€™s not too late â€“ we can add it anyway!

In fact, the above equation for single bits _a_,Â _b_ is _still_ true<sup id="footnote-proof-exercise">6</sup>If youâ€™re mathematically minded and still donâ€™t find this instantly obvious, you might want to try actually _proving_ it. I leave this mostly as an exercise for the reader, but a hint would be: break down each of the input integers _a_ and _b_ as the sum of terms like 2<sup><i>n</i></sup>_a_<sub><i>n</i></sub> (each one being an individual bit of one of the inputs times a power of 2), and collect together the terms on each side that involve the same _n_.6, even when _a_,Â _b_ are whole integers, and the XOR and AND operators are bitwise:

_a_Â +Â _b_Â =Â (_a_Â XORÂ _b_)Â +Â 2Â Ã—Â (_a_Â ANDÂ _b_)

Just to show it in action, hereâ€™s a demonstration set of values, chosen so that the binary values cover a lot of possible bit combinations, but the decimal values make it immediately clear that the final line is also _a_Â +Â _b_:

<table><tbody><tr><td></td><th>Binary</th><th>Hex</th><th>Decimal</th></tr><tr><th><i>a</i></th><td><code>00111101011011000100101001010010</code></td><td><code>3D6C4A52</code></td><td><code>1030507090</code></td></tr><tr><th><i>b</i></th><td><code>00001100001010011011100010000000</code></td><td><code>0C29B880</code></td><td><code>&nbsp;204060800</code></td></tr><tr><th><i>a</i>&nbsp;AND&nbsp;<i>b</i></th><td><code>00001100001010000000100000000000</code></td><td><code>0C280800</code></td><td><code>&nbsp;203950080</code></td></tr><tr><th>2&nbsp;Ã—&nbsp;(<i>a</i>&nbsp;AND&nbsp;<i>b)</i></th><td><code>00011000010100000001000000000000</code></td><td><code>18501000</code></td><td><code>&nbsp;407900160</code></td></tr><tr><th><i>a</i>&nbsp;XOR&nbsp;<i>b</i></th><td><code>00110001010001011111001011010010</code></td><td><code>3145F2D2</code></td><td><code>&nbsp;826667730</code></td></tr><tr><th>(<i>a</i>&nbsp;XOR&nbsp;<i>b</i>)&nbsp;+&nbsp;2&nbsp;Ã—&nbsp;(<i>a</i>&nbsp;AND&nbsp;<i>b)</i></th><td><code>01001001100101100000001011010010</code></td><td><code>499602D2</code></td><td><code>1234567890</code></td></tr></tbody></table>

A demonstration case of adding via AND and XOR

I think of this as the â€œhalf-adder identityâ€ (although as far as I know thatâ€™s not a standard name). The name comes from the hardware design concept of a â€œhalf-adderâ€, which is a small logic component consisting of just an AND and XOR gate, doing addition of two bits in hardware:

![](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/half-adder.svg)

A half-adder implemented with two logic gates

Itâ€™s called a _half_ adder because in order to do full binary addition you need two of these per bit, because you have to add _three_ input bits: two from the input integers and one carried from the bit position to the right. So you build a _full_ adder out of two half-adders, one adding _a_ to _b_ and the second adding the sum of those two to _c_, with one extra gate<sup id="footnote-or-xor">7</sup>The extra gate that combines the carry bits can be either an OR gate or an XOR gate, whichever is easier. It doesnâ€™t matter which, because the two types of gate only disagree in the case where both their inputs are 1, and in this circuit, that canâ€™t happen! If the carry from the first half-adder is 1, then its other output _must_ be 0, which means thereâ€™s no carry from the second half-adder.7 to combine the two carry bits:

![](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/full-adder.svg)

A full adder implemented with two half-adders and an extra gate

â€œBut hang on,â€ you might be thinking. â€œIf you need two half-adders per bit to do a full addition, how is that â€˜half-adder identityâ€™ of yours getting away with just _one_ pair of AND and XOR operations per bit?â€ Or you might think: â€œYouâ€™ve generated an _output_ carry from each bit position, but where have you handled the _input_ carry?â€ Or you might think â€œHang on, how do you arrange for a carry in the lowest bit of the addition to potentially propagate all the way up to the top, when the only left shift in this expression moves data by only one bit?â€

The answer to all three questions is: because of the + sign on the right-hand side of the identity. After we compute the outputs of _one_ half-adder on each pair of input bits, producing a word full of low bits and a word full of carries â€¦ we recombine the two words _using an addition_. Thatâ€™s what finishes the job of propagating carries.

In other words, unlike the hardware half-adder, the â€œhalf-adder identityâ€ _doesnâ€™t_ build an addition out of only simpler operations. It builds an addition out of two simpler operations _and an addition_.

â€œWell, in _that_ case,â€ you might think, â€œisnâ€™t it a complete cheat, and not useful for anything?â€

Not quite! Itâ€™s true that this identity is not _often_ useful. But â€˜not oftenâ€™ isnâ€™t â€˜neverâ€™, and in unusual circumstances there are uses for it.

#### Averaging two integers

Hereâ€™s one occasional use for the half-adder identity.

Suppose you need to calculate the average of two integers, each the size of your CPUâ€™s registers (letâ€™s say, 32 bits). In other words, you want to add two integers, and then divide by 2 (or, equivalently, shift right by 1 bit).

But adding two 32-bit integers makes a 33-bit sum, which doesnâ€™t fit in your register. If you just do the simple thing â€“ add and then shift â€“ then you potentially get the wrong answer, because the topmost bit has been lost in between the addition and the right shift.

Most CPUs have a good answer to this. The add instruction can (perhaps optionally) set a â€˜carry flagâ€™ to indicate whether a 1 bit was carried off the top of an addition; also, thereâ€™s usually a special form of right shift by 1 bit that brings the carry bit back in to the top of the word. (On Arm, that instruction is called RRX; on x86, RCR.) So the way to average two numbers without an overflow problem is to do ADD followed by RRX.

But a few CPUs canâ€™t do it that way. Some architectures donâ€™t have a carry flag at all, or indeed any other flags: for example, MIPS, RISC-V, and the historic DEC Alpha. In other situations, there is a carry flag, but no convenient RRX instruction, so that it would take a lot of effort to recover the carry bit and put it at the top of the shifted-right word; for example, the initial versions of Armâ€™s space-efficient â€œThumbâ€ instruction set discarded the RRX instruction, simply because there wasnâ€™t room for everything.

So, if you _canâ€™t_ do ADD and RRX, how do you most efficiently compute your average?

The half-adder identity provides the answer. If the sum of the two inputs is expressed like this (with the << operation meaning â€˜shift leftâ€™) â€¦

_a_Â +Â _b_Â =Â (_a_Â XORÂ _b_)Â +Â (_a_Â ANDÂ _b_)Â <<Â 1

â€¦ but what you want is the same sum shifted right by one bit, then all you have to do is to shift the XOR term _right_, instead of the AND term left:

(_a_Â +Â _b_)Â >>Â 1Â =Â (_a_Â XORÂ _b_)Â >>Â 1Â +Â (_a_Â ANDÂ _b_)

#### Implementing XOR itself

Also on the theme of â€˜compensating for missing features of your CPUâ€™, hereâ€™s another thing you can do with the half-adder identity.

In the 1970s, Data General made a CPU with such a small instruction set that it didnâ€™t even _implement_ the bitwise XOR operation â€“ it had AND, but not XOR. On that CPU, what would you have done if you needed XOR?

The answer is to use the half-adder identity in reverse! Instead of starting from _a_Â XORÂ _b_ and making _a_Â +Â _b_, do it the other way round. Given the equation

_a_Â +Â _b_Â =Â (_a_Â XORÂ _b_)Â +Â 2Â Ã—Â (_a_Â ANDÂ _b_)

you can rearrange it to get

_a_Â XORÂ _b_Â =Â (_a_Â +Â _b_)Â âˆ’Â 2Â Ã—Â (_a_Â ANDÂ _b_)

So if you have addition and bitwise AND, you can build XOR out of them!

(Exercise for the reader: you can also make bitwise OR in the same way, by removing the â€œ2Â Ã—â€ in the expression â€“ subtract (_a_Â ANDÂ _b_) _without_ doubling it first.)

### Swapping bits in a word

Suppose you have a collection of bits packed into an integer, and you want to swap the positions of two of them. Whatâ€™s the best way?

Thereâ€™s no one answer to that question, because a lot depends on what hardware youâ€™re doing it on (and what useful capabilities it has), and also, on what else you want to do at the same time. For example, swapping a _lot_ of pairs of bits can often be done more efficiently than by swapping one pair at a time. So this is one of these simple-looking questions that becomes surprisingly complicated if you add â€œâ€¦ and do it absolutely as fast as possibleâ€ to the requirements.

But more than one of the good techniques for solving this problem are based on XOR, because you can divide the problem into two essential cases:

-   If the two bits you want to swap are both 0, or both 1, then swapping them _makes no difference anyway_. So in that situation, you donâ€™t need to do anything at all.
-   If one of the bits is 0 and the other is 1, then swapping them _is the same as inverting each one_.

In other words, a reasonable strategy is: first find out if the bits are different, and then, if so, invert both.

Both of those are applications of XOR. If you XOR the two bits with each other, that will give you 1 if theyâ€™re different. And then you can use that value as input to another XOR, using it as a â€˜controlled bit flipâ€™ to invert both bits, but only if they were different to start with.

#### Using XOR and shifts

Hereâ€™s a piece of pseudocode that uses that principle to swap two bits. It assumes that `pos0` and `pos1` are the _positions_ of the two bits to be swapped, with 0 meaning the units bit, 1 the bit to its left, and so on. It also assumes that `pos1 > pos0`.

```
# Constants derived from pos0 and pos1
bit0 = (1 &lt;&lt; pos0)      # an integer with just the bit at pos0 set
distance = pos1 - pos0  # distance between the two bits

# Computation starting from the input value
diff_all = input XOR (input &gt;&gt; distance)
diff_one = diff_all AND bit0
diff_dup = diff_one XOR (diff_one &lt;&lt; distance)
output = input XOR diff_dup
```

Pseudocode to swap two bits by XOR and shifts

(If you need to swap the same pair of bit positions in a lot of inputs, the first two lines can be precomputed just once and reused.)

The first value we compute, `diff_all`, is made by shifting the input right by the distance between the two bit positions. So each bit of `diff_all` tells you the difference between a pair of the input bits separated by that distance.

But weâ€™re only interested in one _particular_ pair of the input bits. So next we compute `diff_one`, which uses bitwise AND to pick out just a single bit of `diff_all`. This will be 0 if the two bits we want to swap are the same; if theyâ€™re different, it will be equal to `bit0` (i.e. it will have a single bit set at position `pos0`).

If we XORed that value back into `input`, it would conditionally flip the lower of the two bits we want to swap. But we want to conditionally flip _both_ of them. So now we duplicate the useful bit of `diff_one` into the other bit position, by shifting it left by the distance between the two bits, and combining that with `diff_one` itself to get `diff_dup`. This has a 1 in _both_ of the target bit positions if the two bits need to be flipped, and is still all 0 otherwise.

So now, XORing that into the input will flip both bits if necessary, and leave them alone otherwise.

This looks like a lot of effort to swap two bits. But one nice thing about it is that itâ€™s just as easy to swap _lots_ of pairs of bits, if every pair is the same distance apart. (That is, swapping bit _i_ with bit _iÂ +Â d_, for multiple different values of _i_ but the same _d_ every time.) In that situation, the only thing you need to change is the step that makes `diff_one` from `diff_all`: instead of ANDing with a _one_\-bit constant, AND with a _multiple_\-bit constant, containing the low bit of every pair you want to swap.

Why might you want to do _that_ in turn? Because thereâ€™s a general technique called a _BeneÅ¡ network_ which lets you encode a completely arbitrary permutation in the form of a series of stages, with each stage swapping a lot of pairs of things separated by the same distance. The distances iterate through powers of 2 and then back again: for example, you might do a set of swaps with distances 16, 8, 4, 2, 1, 2, 4, 8, 16. (Or the other way round if you prefer â€“ it doesnâ€™t really matter _what_ order you go through the distances in.) So if you want to permute the bits of a word in a completely arbitrary way, a computational primitive that swaps a lot of bits at equal separation is just what you want for each stage of a BeneÅ¡ network.

#### Using XOR and just a two-bit word

In the simpler case where youâ€™re only swapping a single pair of bits, here are a couple of simpler (therefore faster) techniques.

Suppose that you have an integer called `bits`, which has exactly two bits set, at the positions you want to swap. What happens if you make the bitwise AND of that value with your input? There are three possibilities:

-   (`input`Â ANDÂ `bits`)Â =Â 0. Both bits are 0, so you donâ€™t need to do anything to swap them.
-   (`input`Â ANDÂ `bits`)Â =Â `bits`. Both bits are 1, so you still donâ€™t need to do anything.
-   Otherwise, the two bits have different values, so you can swap them by XORing the input with `bits`.

On several CPU architectures this kind of test is easier than doing the full business above with shifts â€“ especially because you donâ€™t need the input bit positions specified in multiple ways. For example, many CPU architectures would let you do something like this (though the syntax of the instructions would vary)

```
  tmp = input AND bits         # zero if both bits are 0
  if tmp == 0, jump to â€˜doneâ€™  # in which case we have no work to do
  tmp = tmp XOR bits           # now zero if both bits are 1
  if tmp == 0, jump to â€˜doneâ€™  # so again we have nothing to do
  input = input XOR bits       # in any other case, flip both bits
done:
```

â€˜Machine pseudocodeâ€™ to swap two bits

You could typically do this with about five instructions, because CPUs will typically set a flag as a side effect of each AND or XOR operation to tell you whether the result was zero. Or, if not that, they might have a single instruction to jump to a different location depending on whether a register is zero.

On the 32-bit Arm architecture you can do this in just three instructions, because you donâ€™t need the jumps: instead, you can make the two XOR operations _conditional_, because 32-bit Arm generally lets you make an instruction conditional on the current CPU flags without needing a separate branch instruction to skip past it.

And on x86, you can also do it in three instructions for a completely different reason. x86 has an even stranger processor status flag called the â€˜parity flagâ€™. This is set whenever the result of an operation has _an odd number of 1 bits_. And in this situation, thatâ€™s exactly the case where you need to flip the bits! So you donâ€™t need to test separately for the â€˜both bits 0â€™ and â€˜both bits 1â€™ cases: they can be tested together, by checking if the parity flag says there were an even number of 1 bits in the result of the AND operation.

```
  ANDS    tmp, input, bits
  EORSNE  tmp, tmp, bits
  EORNE   input, input, bits
```

```
  TST     input, bits
  JPE     done
  XOR     input, bits
done:
```

Arm and x86 machine code to swap two bits

### Three XORs make a swap

In a computer program, how do you swap two numbers?

It depends on the platform. Some programming languages have a dedicated function for it, like C++â€™s `std::swap`. Others have a convenient syntax for assigning multiple variables at once: in Rust you can write `(a, b) = (b, a)` (assuming the variables are `mut`), and in Python the same thing works without even needing the brackets. Even some machine languages support swapping two registers: for example, x86 has the XCHG (exchange) instruction.

But suppose youâ€™re in a simpler language without any of those features, like C. Or suppose youâ€™re programming machine code on an architecture without a â€˜swap two registersâ€™ instruction<sup id="footnote-swp">8</sup>There is an Arm instruction called SWP for â€˜swapâ€™, but itâ€™s not what you want for this kind of purpose. It swaps a register with a value in _memory_, not two registers. Itâ€™s not really for ordinary computation: itâ€™s intended for atomically synchronising between threads, which is very hard to get right and well beyond the scope of this article!8. The usual idiom is to use a temporary variable:

```
a_orig = a;
a = b;
b = a_orig;
```

C code for swapping two integers `a` and `b`

You need the temporary variable because in C you can only assign one variable at a time, so after you execute the assignment `a = b`, both variables now have the same value (namely the value originally in `b`), and youâ€™ve lost the previous value of `a` completely â€“ unless youâ€™d saved it in a third location first.

But just occasionally you donâ€™t have a spare register, or at least it would cost you a lot of extra effort to free one up. It turns out that thereâ€™s a different way to swap two values, _without_ needing a spare storage location<sup id="footnote-aliasing">9</sup>However, thereâ€™s a subtle â€œgotchaâ€ about this technique. It works fine as long as the two variables youâ€™re swapping are actually different variables, but it fails if they _alias_ each other. For example, if you use this trick to swap two elements of an array, so that `a` and `b` are replaced with `array[i]` and `array[j]`, then what happens if _i_Â =Â _j_? In that situation you probably wanted the â€˜swapâ€™ of an array element with itself to leave it unchanged. But the triple-XOR swap idiom will turn it into zero, which may well _not_ be what you wanted!9, using three XORs:

```
a = a XOR b;
b = b XOR a;
a = a XOR b;
```

Pseudocode for swapping `a` and `b` using three XORs

Why does _that_ work?!

Itâ€™s difficult to explain this clearly, because we donâ€™t have separate names for the _variables_ weâ€™re mutating, and the _values_ that are stored in them. So letâ€™s start by making up some names. Letâ€™s say that the value that was _originally_ stored in `a` is called `a_orig` (as I already did above in that C snippet). And similarly, the original value of `b` is called `b_orig`.

Then this is what happens with the three XORs:

<table><tbody><tr><td></td><th>Value in <code>a</code></th><th>Value in <code>b</code></th></tr><tr><th>Before doing anything</th><td><code>a_orig</code></td><td><code>b_orig</code></td></tr><tr><th>After <code>a = a&nbsp;XOR&nbsp;b</code></th><td><code>a_orig&nbsp;XOR&nbsp;b_orig</code></td><td><code>b_orig</code></td></tr><tr><th>After <code>b = b&nbsp;XOR&nbsp;a</code></th><td><code>a_orig&nbsp;XOR&nbsp;b_orig</code></td><td><code>a_orig</code></td></tr><tr><th>After <code>a = a&nbsp;XOR&nbsp;b</code></th><td><code>b_orig</code></td><td><code>a_orig</code></td></tr></tbody></table>

Intermediate states of the three-XOR swap algorithm

The basic idea is that there are three values we care about: the two input values, and their bitwise XOR. And if you have three values, one of which is the XOR of the other two, then knowing _any two_ of them is enough to work out the third, because _each_ of them is the XOR of the other two. So at each step of this algorithm, we set one of the variables to the XOR of both variables, and thatâ€™s always a reversible operation (in fact doing exactly the same thing again _would_ reverse it), so _information is never lost_. At every stage, XORing the two values we have in our variables recovers whichever one we currently _donâ€™t_ have. And it takes three XORs to rotate through all the possible options, which is why after three steps weâ€™re back to having the original values of _a_ and _b_ â€“ but the other way round.

I should say at this point that this isnâ€™t a trick you can _only_ do with XOR â€“ it doesnâ€™t depend on XOR having any essential magic that other operations donâ€™t. You can use the same technique with additions and subtractions if you prefer, by setting one variable to the _sum_ of the other two:

```
a = a + b;     // now a = (a_orig + b_orig)
b = a - b;     // now b = (a_orig + b_orig) - b_orig = a_orig
a = a - b;     // now a = (a_orig + b_orig) - a_orig = b_orig
```

Pseudocode for swapping `a` and `b` by addition and subtraction

But the XOR version is more common (among people who want to do this trick at all). Partly thatâ€™s because itâ€™s simpler: all three operations are the same, whereas in the additive version all _three_ operations are different (the two subtractions are opposite ways round, in the sense of which of their inputs they overwrite).

But also, the XOR version is more flexible, because â€“ just like in the [pixel graphics](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/pixels) application â€“ you can use it in situations where addition would cause unwanted extra carries.

For example, hereâ€™s a case where you can swap with three XORs, which wouldnâ€™t work with three additions. Suppose that `a` and `b` are 32-bit registers, and `n` is some number less than 32:

```
a = a XOR (b &lt;&lt; n);
b = b XOR (a &gt;&gt; n);
a = a XOR (b &lt;&lt; n);
```

Swap part of `a` with part of `b`

The effect of this code is to swap _parts_ of the two values. Specifically, only the upper (32Â âˆ’Â _n_) bits of `a` are affected, because every time we XOR something into `a`, the value we XOR in has been shifted left by _n_ bits, so its lower _n_ bits are zero. Similarly, the value XORed into `b` has been shifted right _n_ bits, so its top _n_ bits are zero, so the operation only affects the lower (32Â âˆ’Â _n_) bits of `b`.

![](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/shifted-swap.svg)

The parts of `a` and `b` affected by the above code

In fact, this code precisely swaps those two segments of the inputs: the upper (32Â âˆ’Â _n_) bits of `a` are swapped with the lower (32Â âˆ’Â _n_) bits of `b`.

One use for this trick Iâ€™ve seen in the past: suppose that you were operating on a buffer of graphics data with one byte per pixel. Youâ€™ve loaded a word from the buffer containing four pixels _PQRS_, and youâ€™re trying to magnify it by a factor of two, so that you want to create words containing _PPQQ_ and _RRSS_.

The neatest way I know to do it is to use this â€˜shifted swapâ€™ technique twice:

-   Initialise both `a` and `b` to copies of the input word _PQRS_.
-   Do a shifted swap with _n_Â =Â 8, swapping the top 24 bits of `b` (containing _PQR_) with the bottom 24 of `a` (containing _QRS_). Now `a`Â =Â _PPQR_, and `b`Â =Â _QRSS_.
-   Do a second shifted swap, this time with _n_Â =Â 24, swapping the top 8 bits of `b` (containing _Q_) with the bottom 8 of `a` (containing _R_). Now `a`Â =Â _PPQQ_, and `b`Â =Â _RRSS_. Done!

### Winning at the game of Nim

Possibly the _strangest_ application of XOR that I know of is in game theory.

In the simple combinatorial game of â€˜Nimâ€™, there are some piles of counters. On your turn, you must choose one pile and remove any number of counters you like from it, from 1 to the whole pile, or anywhere in between (but not 0). You lose if you have no possible move<sup id="footnote-misere">10</sup>In fact, Nim comes in two well-known forms, with opposite win conditions. In the variant discussed here, your aim is to take the last counter and leave your opponent with no move. In the other variant, the win condition is exactly the reverse: you aim to make your _opponent_ take the last counter! The strategy for the latter is a tiny bit more complicated, but not too bad. I only discuss the simple version here.10, which only happens if there are no counters at all remaining (because your opponent took the last one).

How do you determine a good move in this game?

![](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/nim-example.svg)

The unique winning move here is to take 3 counters from the pile of 12. But why?

The basic idea is to identify a set of _losing positions_: states of the game in which, if itâ€™s your turn, youâ€™re in trouble. This set should have the properties that:

1.  the position where you have actually lost counts as a losing position. (Itâ€™d be embarrassing to get that wrong.)
2.  from any losing position, _every_ possible move leaves the other player in a non-losing position. (If youâ€™re in trouble, you stay in trouble no matter what you do.)
3.  from any non-losing position, there should be _at least one_ move which leaves the other player in a losing position. (If you have the advantage, thereâ€™s a way to keep it, though you may have to think about it.)

For a game like Nim, where every move reduces the total number of counters, you could analyse the game computationally by iterating through all the possible positions in order of increasing size, and for each one, classify it as â€˜losingâ€™ or â€˜non-losingâ€™ according to the above rules, by checking the results of smaller positions youâ€™ve already classified. Of course this only gets you the status of a finite number of positions (until you run out of patience to run the computer program), but you might hope to see a pattern, which you could then try to prove.

If you try this, it turns out that there is indeed a pattern, and itâ€™s a surprising one. The losing positions in Nim are precisely positions in which _the bitwise XOR of all the pile sizes is zero!_

Two of the three criteria above are easy to check:

1.  If there are no counters at all, then XORing all the pile sizes together gives you zero, because there arenâ€™t any piles at all (or, equivalently, they all have zero height). So the â€˜you have lostâ€™ position is a losing position by this XOR rule. âœ“
2.  If youâ€™re in a losing position (all the piles XOR to zero), then your move must change exactly one of the pile sizes, say from _a_ to _b_. So the XOR of the pile sizes has changed by _a_Â XORÂ _b_, which (since _a_Â â‰ Â _b_) isnâ€™t zero. So any move from a losing position goes to a non-losing position. âœ“

The third condition is the trickiest. From a _non_\-losing position â€“ if the piles XOR to some nonzero value _x_ â€“ there always needs to be _at least one_ move that makes the piles XOR to zero again.

If you were allowed to _add_ counters to a pile as well as removing them, this would be easy. Just modify any pile you like by XORing its size with _x_ (which will necessarily change it from its previous value), and then the sizes XOR to zero.

But in some cases, this makes a pile bigger, and thatâ€™s not allowed. So we need to show that thereâ€™s at least one pile that is made _smaller_ by doing this.

First, how can you tell whether XORing some other number _n_ with _x_ makes it bigger or smaller? The answer is to look at the _highest_ 1 bit in _x_. If that bit is also 1 in _n_, it will be 0 in _n_Â XORÂ _x_, and that means _n_Â XORÂ _x_ will be smaller than _n_. This is true _no matter_ what the lower-order bits do, because even if all the lower bits change from 0 to 1, the sum of those effects will still be (just) smaller than the effect of the high bit changing from 1 to 0.

So, if the piles XOR to _x_, then weâ€™re looking for pile sizes which have a 1 in the same place as the highest 1 bit of _x_. Those are exactly the piles for which XORing _x_ into the size will make it smaller â€“ meaning we can modify that pile in a way that is both within the rules, and creates a losing position for the other player.

So we can find a legal winning move if thereâ€™s at least one pile size with a 1 in that bit position. But of course there must be one, _because_ that bit is set in _x_: if every pile size has 0 in a given bit position, then _x_ does too!

For an example, letâ€™s calculate whatâ€™s going on in the picture above. The three pile sizes are 12, 10 and 3. In binary, those are 1100, 1010 and 0011. The bitwise XOR of those values is 0101, so this isnâ€™t a losing position. To win, we must change a pile size by XORing it with 0101. Two of the pile sizes would be made bigger by this operation: the two smaller sizes, 1010 and 0011, have a 0 in the relevant position (theyâ€™re of the form _x_0_yz_), so they would become 1111 and 0110 respectively, each larger than their original size. But the largest pile, with size 1100, has that bit set, so it becomes 1001, a smaller value. Therefore, thereâ€™s only one winning move, and itâ€™s to reduce the largest pile from size 12 to size 9 by removing three counters, as the picture shows.

One reason I find this a particularly strange place for bitwise XOR to show up is that it doesnâ€™t to have anything to do with your choice of number base. If youâ€™re writing two integers _in binary_, then it might seem very natural to combine corresponding pairs of digits in various ways. But if you write the same integers in base 10, or some other base like 3 or 5, then youâ€™d find yourself imagining a totally different set of â€˜digit-wiseâ€™ operations analogous to the bitwise ones (like taking the minimum, or maximum, or sum mod 10, of each pair of corresponding decimal digits). So youâ€™d expect bitwise XOR to show up in situations where it was important that youâ€™d written a number in binary. But the winning strategy in Nim doesnâ€™t depend on what base you write the pile sizes in, or even whether you wrote them down in place-value notation at all â€“ so the appearance of bitwise XOR _seems_ to be saying that binary is important to the underlying mathematics, whether humans have thought of it or not!

## Mathematical concepts that look like XOR

That last example, from game theory, is moving more in the direction of mathematics rather than practical computing. So this is a good moment to change direction and talk about some concepts in pure mathematics that are basically XOR with a different name, or in a not-very-subtle disguise.

In some cases, thereâ€™s a whole further area of study that follows on from the XOR-like operation, showing that XOR isnâ€™t just a useful thing in its own right â€“ itâ€™s also the starting point of a lot more.

In the following sections the mathematics will be more advanced than itâ€™s been until now: I donâ€™t have space to describe every concept from absolute scratch, so in each section Iâ€™ll have to rely on some background knowledge that makes it possible to explain the new concept briefly.

If that isnâ€™t your thing, then I hope youâ€™ve enjoyed the previous parts of the article!

### Symmetric difference of sets

Iâ€™ll start with a simple one. Thereâ€™s a natural correspondence between Boolean logic operations, and operations on sets in set theory. For any set _X_, you can imagine asking the yes/no question â€˜Is this particular thing a member of _X_?â€™. Then, set operations on the sets themselves (like union and intersection) correspond naturally to Boolean logic operations on the answers to those membership queries.

For example, if you have two sets _X_ and _Y_, then when is some element _a_ in the union _X_Â âˆªÂ _Y_? Precisely when either _a_ is in _X_, _or_ _a_ is in _Y_, or both. The union operator corresponds to the Boolean (inclusive) OR.

Similarly, _a_Â âˆˆÂ (_X_Â âˆ©Â _Y_) precisely when _a_Â âˆˆÂ _X_ _and_ _a_Â âˆˆÂ _Y_. The intersection operator corresponds to AND.

In this model, what corresponds to XOR? Itâ€™s the _symmetric difference_ operator, written _X_Â âˆ†Â _Y_: the set of elements that are in _exactly one_ of _X_ and _Y_, no matter which one it is. _a_Â âˆˆÂ (_X_Â âˆ†Â _Y_) precisely when (_a_Â âˆˆÂ _X_) XOR (_a_Â âˆˆÂ _Y_).

This correspondence between XOR and symmetric difference means that the âˆ† operator has all the same properties as XOR â€“ for example, itâ€™s both commutative and associative. Proving this is a common introductory exercise in simple set theory, and doing it directly can easily lead to half a page of tedious algebra; but understanding symmetric difference as â€˜basically XORâ€™, and XOR in turn as the same thing as addition mod 2, makes it clear that symmetric difference inherits commutativity and associativity from addition itself.

### Groups of exponent 2

In group theory, if _g_ is an element of a group _G_, you can ask: is any power of _g_ equal to the group identity _e_? If so, whatâ€™s the _smallest_ number _n_Â >Â 0 such that _g_<sup><i>n</i></sup>Â =Â _e_? This number is called the _order_ of the element _g_. (If there is no such integer _n_, so that all the powers of _g_ are different, then we say that _g_ has infinite order.)

Instead of asking about the order of one specific element of _G_ at a time, you can also ask a similar question for the whole group at once: is there a single number _n_ such that, for _every_ element _g_Â âˆˆÂ _G_, _g_<sup><i>n</i></sup>Â =Â _e_? The smallest such number is called the _exponent_ of the group _G_. (Again, it may be infinite. If itâ€™s finite, then itâ€™s also the lowest common multiple of all the orders of individual elements.)

If a group has exponent 2 in particular, then that means every element is _self-inverse_: _g_<sup>2</sup>Â =Â _e_ for all _g_. A standard exercise is to show that this also makes the group _abelian_, i.e. the group operation is commutative, i.e. _gh_Â =Â _hg_ for all _g_,_h_.

Group operations are also associative, by definition of a group. So in this situation, we have an operation thatâ€™s commutative, associative, has an identity (namely _e_), and everything is self-inverse. So if you have a long list of group elements combined together, you can reorder it to bring identical elements together, and then any two copies of the same element cancel out.

That sounds a lot like XOR â€“ and it is. Every group of exponent 2 can be understood as a special case of XOR, by imagining that each element of the group corresponds to a function (on some set _X_ that depends on the group) taking values in {0,Â 1}, and combining two elements has the effect of taking the bitwise XOR (or sum mod 2) of their associated functions.

Not every such group contains _all_ the possible functions from _X_Â â†’Â {0,Â 1}. Every _finite_ group of exponent 2 does (as long as you donâ€™t define _X_ to have spare unused elements), but infinite groups can be more subtle. You might have, for example, the set of functions from â„•Â â†’Â {0,Â 1} that are eventually all 0, or eventually constant, or eventually periodic.

But all groups of exponent 2 correspond to _some_ set of functions with codomain {0,Â 1}, under bitwise XOR.

### Nim-sum

In an [earlier section](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#nim) we saw that the losing positions in the game of Nim are characterised by the pile sizes XORing to zero.

This isnâ€™t just an isolated mathematical curiosity about one obscure game. Nim is central to the theory of _Sprague-Grundy analysis_, which proves a large class of other games to be â€˜equivalentâ€™ to Nim in the sense that you can analyse them using the same technique.

However, the class of games that this works for doesnâ€™t include most games you might normally play. Itâ€™s limited to _impartial_ games, which are those where the set of permitted moves donâ€™t change depending on which playerâ€™s turn it is. Chess, for example, is _not_ impartial, because each piece belongs to a specific player, and the other player isnâ€™t allowed to move it. Itâ€™s not enough that one playerâ€™s pieces are basically the same as the other playerâ€™s, and move by the same rules: chess would only be impartial if both players were allowed to move _any piece they liked_, regardless of colour.

The basic idea is that for any position _P_ in any impartial game, you can assign it a number _n_, known as a _Grundy number_. Then you can treat position _P_ as â€˜essentiallyâ€™ the same as a Nim pile containing _n_ counters, in the sense that for every smaller number _m_Â <Â _n_, thereâ€™s a move that turns _P_ into a position with Grundy number _m_, but no move that leaves the Grundy number unchanged at _n_ itself. (There may or may not be moves that go to _larger_ numbers; in that respect Grundy numbers differ from actual Nim piles, but this difference turns out not to matter.)

In many of these games, itâ€™s natural to combine multiple instances of the game into one bigger one, in a way that gives the player the choice of which subgame to make a move in. This operation is called making a _composite_, or sometimes a _disjunctive sum_, of the smaller games. For example, a Nim game with multiple piles of counters is exactly the composite of smaller Nim games each containing just one of the piles, because on your turn you must choose just one of the subgames (piles) to modify, and make a move that would be legal in that subgame by itself.

How do you work out the Grundy number of a composite game? One very convenient way is to first find the Grundy number of each component (which are smaller and simpler games, so this is usually easier). Then the overall game is a composite of smaller games, each one equivalent to a Nim pile of a particular size â€“ and so its Grundy number is obtained the same way youâ€™d evaluate a position in Nim, by combining the smaller gamesâ€™ Grundy numbers using bitwise XOR.

A concrete example is the game of â€˜kaylesâ€™, which starts off with a row of counters equally spaced, and on your move you may remove any single counter, or two counters directly adjacent. So most of the possible first moves divide the starting row into two smaller rows, which you then have to play in separately (thereâ€™s no move you can make that affects both). Sprague-Grundy analysis saves you from having to analyse every possible _combination_ of kayles rows: instead, you only need to work out the Grundy number of each length of _single_ kayles row, and then the Grundy number of a composite of multiple rows can be worked out by XOR.

So bitwise XOR is crucial to this entire branch of game theory. For this reason, the operation of bitwise XOR on non-negative integers is sometimes referred to by game theorists as the _nim-sum_.

### _GF_(2), the finite field of order 2

A _field_, in mathematics, means an algebraic structure in which you can add, subtract, multiply and divide any two numbers (except for dividing by 0), and you still get a number within the original field, and those operations behave â€˜sensiblyâ€™ in the same ways youâ€™re used to, both individually and in combination with each other. For example, you expect _a_Â +Â _b_Â =Â _b_Â +Â _a_, and _a_Â âˆ’Â _a_Â =Â 0, and _a_(_b_Â +Â _c_)Â =Â _ab_Â +Â _ac_.

Well-known fields include the real numbers, â„; their superset, the complex numbers â„‚; their subset, the rational numbers â„š. There are also a great many intermediate fields between â„š and â„, such as the set of all numbers of the form _a_Â +Â _b_âˆš2 for rational _a_ and _b_. A well-known example of something thatâ€™s _not_ a field is the integers, â„¤: you can add, subtract and multiply integers just fine and still get an integer, but if you divide two integers, you can easily get something that isnâ€™t an integer any more.

All of those fields have infinite size. If nothing else, they contain the integers: you can start from 1, and keep adding 1, and get an endless sequence of numbers, all different and all still in the field.

But thereâ€™s also such a thing as a _finite_ field: a structure that obeys all the same rules as any other field, but has a finite number of different elements.

A finite field has a fundamentally different nature from the fields Iâ€™ve mentioned so far. If you do that same experiment I just mentioned in a finite field â€“ start with 1 and keep adding 1 â€“ you _canâ€™t_ get an infinite sequence of different values, because there arenâ€™t infinitely _many_ different values at all. Sooner or later, you must repeat a number youâ€™ve run into before.

In particular, this means that if you count up 1, 2, 3, â€¦ in a finite field, you must at some point find that one of those numbers is equal to _zero_ again. So finite fields have the nature of _modular_ arithmetic, rather than ordinary arithmetic: thereâ€™s always some positive number _p_ (known as the _characteristic_ of the field, and as it turns out, always prime) such that adding 1 to itself _p_ times gives zero, and therefore, adding _anything_ to itself _p_ times also gives zero<sup id="footnote-infinite-positive-characteristic">11</sup>However, the converse isnâ€™t true. Any finite field has positive characteristic, but not every field with positive characteristic is finite. There are also _infinite_ fields with this same modular-arithmetic property. But we wonâ€™t get as far as discussing those here.11. (Which means that you also arenâ€™t allowed to divide by _p_, because you canâ€™t divide by zero, and _p_ is the same thing as zero in this context.)

In fact, the simplest example of a finite field is _precisely_ modular arithmetic. For a prime _p_, the integers mod _p_ have all the properties of a field, as long as you interpret â€˜dividing by _n_â€™ to mean multiplying by the modular inverse of _n_ mod _p_.

And the simplest example of _that_ is to take _p_ to be the smallest prime of all, namely 2. If you do that, you get a field with just two numbers in it: 0 and 1! This field is called various things, including _GF_(2) and ğ”½<sub>2</sub>. (â€˜GFâ€™ stands for â€˜Galois fieldâ€™, after the mathematician who pioneered research in this area.)

This field is so small that itâ€™s possible to just _list_ all the answers to every basic arithmetic operation:

-   Addition works mod 2, so 1Â +Â 1Â =Â 0. Every other addition is the same as in normal integers: 0Â +Â 0Â =Â 0 and 0Â +Â 1Â =Â 1.
-   Subtraction is _exactly the same thing as addition_, because âˆ’1 and +1 are the same thing in any â€˜mod 2â€™ context. In particular, 0Â âˆ’Â 1Â =Â 1 (again, because âˆ’1Â =Â +1), and everything else is the same as in normal integers: 0Â âˆ’Â 0Â =Â 0, 1Â âˆ’Â 1Â =Â 0 and 1Â âˆ’Â 0Â =Â 1.
-   Multiplication works _exactly_ like normal integers: any multiplication involving 0 gives 0, and 1Â Ã—Â 1Â =Â 1.
-   The only case of division thatâ€™s allowed _at all_ is dividing a number by 1, which just gives you the same number back again. And you canâ€™t divide by 0 at all.

To put this another way: the elements of this field look like booleans (with the usual convention of 0Â =Â false and 1Â =Â true), and addition and subtraction both behave like the XOR operator. Multiplication behaves like AND: the product is 1 only if both inputs are 1, because otherwise, at least one input is 0, and multiplying by 0 gives 0.

This means that weâ€™ve just found out another algebraic property of the XOR operator: AND distributes over it, which is to say that

_a_Â ANDÂ (_b_Â XORÂ _c_)Â =Â (_a_Â ANDÂ _b_)Â XORÂ (_a_Â ANDÂ _c_)

because thatâ€™s just the translation into Boolean algebra of the ordinary algebraic identity _a_(_b_Â +Â _c_)Â =Â _ab_Â +Â _ac_, which is true in _GF_(2) just like in any other field.

This tiny field seems as if itâ€™s surely too trivial to actually be useful for anything. But it isnâ€™t!

#### Linear algebra over _GF_(2)

All of linear algebra â€“ vectors, matrices, and all that â€“ starts from the definition of a _vector space_. That in turn depends on the starting point of choosing a _field_ which will act as the â€˜scalarsâ€™ of your vector space, and the elements of your matrices. Depending on the field you choose, the vectors and matrices behave differently. (For example, rational, real and complex matrices will disagree on whether a matrix is diagonalisable, or has a square root.)

You can make a vector space over any field you like. Even over the trivially simple _GF_(2), if you want to. If you do that, then vectors are particularly simple: each vector looks like a sequence of numbers which are all either 0 or 1. You could imagine representing this as just a string of individual bits in a computer.

When you add two vectors or matrices, you add each component separately, using whatever addition is appropriate to your field. If the field is _GF_(2), that means the addition is mod 2, i.e. it works like XOR, independently in each component. _Adding two vectors or matrices over_ _GF_(2) _corresponds exactly to bitwise XOR._

What about multiplying a matrix _M_ by a vector _v_? In ordinary real-number linear algebra, one way to look at this is that the output is a linear combination of the columns of the matrix _M_, and the coefficients of the linear combination are given by the components of the vector _v_. That is, the _i_th column of _M_ is multiplied by the _i_th component of _v_, and all those products are added together.

Over _GF_(2), this is particularly simple, because the components of _v_ are all either 0 or 1, so multiplying one of those into a column of _M_ either zeroes it out completely, or leaves it unchanaged. So _v_ is just specifying a _subset_ of the columns of _M_. And then those columns are added together like vectors over _GF_(2), i.e. combined as if by bitwise XOR.

Of course, you have to ask why anyone would bother. Whatâ€™s the use of vectors and matrices in which the components work mod 2 in this way? They clearly donâ€™t represent anything in geometry (like vectors over â„ do), or anything in quantum mechanics or signal processing (which are both applications of vectors over â„‚). Is this just a mathematical curiosity not ruled out by the definition of a vector space, or are there uses for it?

##### Error-correcting codes

There are! And hereâ€™s an example.

If youâ€™re communicating over a noisy channel like a radio, you often want to transmit your data with some redundancy, so that if a few bits of your message arenâ€™t received correctly at the other end, the receiver can tell that it happened, and perhaps even reconstruct the correct message in spite of the errors.

This idea in general is known as an _error-correcting code_. The general idea is that you expand an original message of (say) _m_ bits into some larger number of bits _n_Â >Â _m_ which you send, and then the receiver decodes the _n_ bits they receive to get back your original _m_\-bit message. So there are only 2<sup><i>m</i></sup> strings you might have fed to the encoder as input, and therefore only 2<sup><i>m</i></sup> of the 2<sup><i>n</i></sup> possible _n_\-bit strings could have been produced as output. The idea of an error-correcting code is to â€˜space outâ€™ the valid _n_\-bit codewords in the overall space of _n_\-bit strings, so that any two valid codewords differ in a large number of bits, and a small number of errors canâ€™t turn one into another. If any two valid codewords differ by at least _k_ bits, for example, then a transmission error that alters fewer than _k_ bits can be _detected_ (the receiver recognises that the received string isnâ€™t a valid codeword), and an error altering fewer than _k_Â /Â 2 bits can be corrected, by finding the _nearest_ valid codeword.

(Incidentally, â€˜codesâ€™ in this sense arenâ€™t _secret_ codes, like in cryptography. The word â€˜codeâ€™ in this context has the wider meaning of â€˜any way to convert your message into something convenient to send, so that the receiver knows how to get the message back at the other endâ€™. For this application, we donâ€™t mind if other people can _also_ reconstruct the message. Of course, if you wanted to protect the message against eavesdroppers _and_ against transmission errors, you might put a layer of encryption inside your error-correcting code!)

One very popular way to construct these codes is to make them _linear_, which means that the code basically works by having a pair of matrices over _GF_(2), so that each one takes an input string of bits (represented as a vector), and outputs another string of bits:

-   A _generator matrix_ is used by the sender, to expand the original _m_\-bit message into a longer _n_\-bit codeword.
-   A _check matrix_ is used by the receiver, to find out whether the received codeword is valid. Any valid codeword multiplied by the check matrix should give the zero vector â€“ but if the codeword has errors, then the output vector (known as the _syndrome_) contains enough information to identify them.

This is a convenient system because doing it with vectors and matrices makes the syndrome independent of the message. That is, if the same pattern of error bits occurs in two different messages, both of them generate the same syndrome vector. So the receiver only needs a lookup table that maps every possible syndrome to the pattern of error bits it generates â€“ they _donâ€™t_ need a separate version of that table for each possible codeword.

And in a few cases you donâ€™t _even_ need the lookup table. Hereâ€™s a particularly pretty concrete example of a linear code, known as a _Hamming code_:

Suppose _n_, the length of the code, is one less than a power of 2. For this example Iâ€™ll take _n_Â =Â 15, but any other number of this form (3, 7, 15, 31, 63, â€¦) also works.

Weâ€™ll start by saying what the _receiver_ does. The receiver has a 15-bit string to analyse. They index the bits with all the non-zero 4-bit binary numbers, so that the bits are numbered 0001, 0010, 0011, â€¦, 1110, 1111. Then they XOR together the indexes of all the 1 bits in the message. Legal codewords are defined to be any bit string for which the result of this XOR operation is zero.

So, if you take a legal codeword and flip any single bit, the result of this XOR process is not zero, and better than that, it directly tells you the index of the bit that was flipped! So a Hamming code can correct any one-bit error in a codeword, without even needing a lookup table.

How does the sender construct a codeword? The easiest way is to reserve the bits with power-of-2 indices as check bits, and fill in all the rest. So the 11 bits corresponding to indices that _arenâ€™t_ 1, 2, 4 or 8 are your data bits, which you fill in with the actual message. Then the sender XORs together the indices of all the 1 bits so far, and sets the final four bits however is necessary to make the result become zero: if the lowest bit in the XOR value is 1, they set the bit with index 1 to cancel it out, and the same for the other three bit positions.

So a 15-bit Hamming code lets you transmit 11 bits of actual data, and uses the other 4 bits to allow a one-bit error to be corrected. In general, a (2<sup><i>d</i></sup>Â âˆ’Â 1)-bit Hamming code carries 2<sup><i>d</i></sup>Â âˆ’Â 1Â âˆ’Â _d_ bits of data, using the other _d_ bits to correct a one-bit error. In other words, a longer Hamming code is more efficient (more useful message data per bit transmitted), but correspondingly less good at correcting errors (still only one error allowed per codeword, but the codewords are longer).

This is a linear code, because both the encoding and checking processes Iâ€™ve described can be written down as a matrix over _GF_(2). The process of XORing together the indices of set bits in the received data is exactly the same thing as multiplying by a matrix whose columns contain the nonzero binary integers 0001, 0010, â€¦, 1111. And the process of constructing a message is the same thing as multiplying by a matrix in which each column sets one of the bits with a non-power-of-2 index plus whatever power-of-2 bits cancel it out â€“ for example, there will be a column that sets the bit with index 0101, and then cancels it by also setting the bits 0001 and 0100.

#### Polynomials over _GF_(2)

Another thing we can do with any field, including _GF_(2), is to consider the set of _polynomials_ with coefficients in that field.

That is, we consider expressions of the form

_a_<sub>0</sub>Â +Â _a_<sub>1</sub>_x_Â +Â _a_<sub>2</sub>_x_<sup>2</sup>Â +Â â€¦Â +Â _a_<sub><i>n</i></sub>_x_<sup><i>n</i></sup>

in which the numbers _a_<sub><i>i</i></sub> are all either 0 or 1, and _x_ is an abstract symbol that doesnâ€™t represent an actual number at all<sup id="footnote-formal-polynomials">12</sup>You might argue: if _x_ is a number in _GF_(2), then itâ€™s either 0 or 1, and in either case, _x_<sup>2</sup>, or _x_<sup>3</sup>, or any higher power of _x_, are all equal to _x_ itself. So under that assumption, any polynomial of this kind could be simplified by squashing all the higher-order terms down into the _x_<sup>1</sup> term. But we _donâ€™t_ do that here, because we donâ€™t make the assumption that _x_ is one of the two numbers we know about: we leave open the possibility that any two powers _x_<sup><i>i</i></sup> and _x_<sup><i>j</i></sup> might be unequal.12.

If you want to add or multiply two of these polynomials, you do it exactly as if you were manipulating ordinary polynomials with integer or real coefficients: simplify using the rule _x_<sup><i>i</i></sup>_x_<sup><i>j</i></sup>Â =Â _x_<sup><i>i</i>&nbsp;+&nbsp;<i>j</i></sup>, collect together terms with the same power of _x_, and evaluate each coefficient. The only difference is that the final evaluation happens in _GF_(2), which is equivalent to saying â€˜after you calculate the coefficients, reduce each one mod 2â€™.

For example, suppose you wanted to add the polynomials (1Â +Â _x_Â +Â _x_<sup>2</sup>) and (_x_Â +Â _x_<sup>3</sup>). In the ordinary integers, the sum would be 1Â +Â _2x_Â +Â _x_<sup>2</sup>Â +Â _x_<sup>3</sup>. Over _GF_(2), the answer is exactly the same except that the 2_x_ term vanishes, because weâ€™re working mod 2, so 2 is just the same thing as 0. So you just get 1Â +Â _x_<sup>2</sup>Â +Â _x_<sup>3</sup>.

In other words, if _P_ and _Q_ are two polynomials, then the _x_<sup><i>i</i></sup> term of _P_Â +Â _Q_ is simply the sum of the _x_<sup><i>i</i></sup> terms in _P_ and _Q_ themselves. But the sum is mod 2, i.e. it corresponds to XOR. _Each coefficient of the sum is the XOR of the same coefficient in the two inputs._

In other words, if you consider a polynomial to be represented by just its sequence of coefficients, then _addition of polynomials over_ _GF_(2) _corresponds exactly to bitwise XOR_.

What about multiplication? The same rule works: multiply the polynomials the same way you would normally, and then reduce the coefficients mod 2. So the product (1Â +Â _x_Â +Â _x_<sup>2</sup>)(_x_Â +Â _x_<sup>3</sup>), for example, would normally work out to _x_Â +Â _x_<sup>2</sup>Â +Â 2_x_<sup>3</sup>Â +Â _x_<sup>4</sup>Â +Â _x_<sup>5</sup>, but again, reducing the coefficients mod 2 makes the 2_x_<sup>3</sup> term vanish. So over _GF_(2), the product is _x_Â +Â _x_<sup>2</sup>Â +Â _x_<sup>4</sup>Â +Â _x_<sup>5</sup>.

Another way to describe this algorithm is: to calculate the product of two polynomials _P_ and _Q_, for each term _x_<sup><i>i</i></sup> in _P_, make the partial product _Q__x_<sup><i>i</i></sup>, which looks just like _Q_ itself except that itâ€™s â€˜shifted upwardsâ€™ by _i_ places â€“ the power of _x_ in each term is larger by _i_ than it originally was. Then add together all of those partial products, in â€˜mod 2â€™ fashion.

This looks very similar to the algorithm you use for multiplying two ordinary integers _a_ and _b_ if youâ€™re given them written in binary. In that algorithm, you again make a shifted value _b_Â Ã—Â 2<sup><i>i</i></sup> for every power of 2 corresponding to a 1 bit in _a_. The only difference is that you combine the partial products at the end using ordinary integer addition, instead of bitwise XOR.

In software that deals with polynomials over _GF_(2) a lot, itâ€™s actually very convenient to _represent_ one of these polynomials as a binary number, with the bit that would normally have value 2<sup><i>i</i></sup> instead being taken to be the coefficient of _x_<sup><i>i</i></sup>. For example, you might represent 1Â +Â _x_<sup>2</sup>Â +Â _x_<sup>3</sup> using the integer 1Â +Â 2<sup>2</sup>Â +Â 2<sup>3</sup>Â =Â 13. (As if youâ€™d forgotten about the â€˜mod 2â€™ business and just evaluated the original polynomial at _x_Â =Â 2.)

In that representation, multiplying polynomials looks almost exactly like multiplying integers in binary â€“ you make a shifted copy of one of the inputs _a_ for each set bit of _b_, and then combine them all â€“ except that combining all the values at the end is done â€˜without carryingâ€™, i.e. itâ€™s replaced with XOR. Some CPU architectures even provide a built-in hardware instruction to do this modified type of multiplication. The x86 architecture calls it a name like â€˜carryless multiplicationâ€™, with instruction names including the string `CLMUL`; the Arm architecture calls it â€˜polynomial multiplicationâ€™, and youâ€™ll probably find a P in the name of any instruction that does it.

##### CRCs

These polynomials over _GF_(2) behave in some ways similarly to the integers: you can add, subtract and multiply them as much as you like, but when you try to divide two polynomials, the result often isnâ€™t still a polynomial, and you have to decide what to do about that.

Just like dividing in the integers, one thing you can do is to deliver a quotient and a remainder: if youâ€™re asked to calculate _P_Â /Â _Q_ and you find that _P_ isnâ€™t a multiple of _Q_, you can find a nearby polynomial that _is_ a multiple of _Q_, and return the result of dividing _that_ by _Q_, plus the â€˜remainderâ€™ thatâ€™s the difference between _P_ itself and the polynomial you substituted.

A well-known kind of checksum, used to verify transmission of network packets in Ethernet and for many other similar purposes, is called a Cyclic Redundancy Check (CRC), and it works like this:

-   Choose a polynomial _P_ over _GF_(2). (There are several polynomials people like to use, typically with degree 32 or sometimes 16. But for a given application, both sender and receiver must agree on a specific one.)
-   Given a message to transmit, expressed as a sequence of bits, pretend that the entire message is itself a giant polynomial _M_ over _GF_(2), with the coefficients given by the bits of the message.
-   Divide _M_ by _P_, throw away the quotient, and keep the remainder. That is, reduce _M_ mod _P_.

You can see this, in some ways, as very similar to the error-correcting codes I mentioned in a [previous section](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#ecc). A CRC will only _detect_ errors, not correct them; and because itâ€™s a tiny number of bits appended to a huge message, its power to detect errors is much smaller than those more rigorous codes. If youâ€™re trying to transmit over a noisy radio channel then you probably use full error-correcting codes; CRCs are for the kind of situation where _almost_ every transmission is free of error, and very rarely thereâ€™s either one flipped bit or a sudden burst of random noise, and itâ€™s fine to deal with the problem by telling the other end â€˜whoops, that one packet didnâ€™t arrive intact, please re-send itâ€™.

##### Making larger finite fields

In a [previous section](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#gf2poly) I said that the _simplest_ examples of finite fields consisted of the integers mod _p_, for some prime _p_. But theyâ€™re not the only examples.

Let me describe the process of making one of these simplest finite fields _GF_(_p_) in slightly more detail:

-   Start with the integers.
-   Choose a specific value _p_ which isnâ€™t divisible by anything smaller.
-   Reduce everything else mod _p_ â€“ that is, consider things as the same if they differ by a multiple of _p_. This leaves only finitely many things counted as different.
-   Because _p_ is prime, it turns out that there is now always a multiplicative inverse of any value not equivalent to 0. So division now works, and weâ€™ve made a field.

It turns out that all the _other_ finite fields that exist can be made by following exactly the same procedure a second time â€“ except that instead of starting with the integers, you start with the set of polynomials over _GF_(_p_).

That is: first pick _p_ and make the finite field of integers mod _p_. Now pick a polynomial _Q_ with coefficients in _GF_(_p_), which is _irreducible_ â€“ that is, it isnâ€™t the product of any two smaller polynomials â€“ and reduce all the rest of the polynomials over _GF_(_p_) mod _Q_.

The result is always a finite field. It still has characteristic _p_ â€“ that is, adding together _p_ copies of the same thing gives zero. But it has more than _p_ elements. In fact, if the polynomial _Q_ has degree _d_ (that is, its highest-order term is _x_<sup><i>d</i></sup>), then the new field has _p_<sup><i>d</i></sup> elements<sup id="footnote-unique-finite-fields">13</sup>In fact, it turns out that _all_ finite fields of a given size _p_<sup><i>d</i></sup> are the same as each other. Choosing a different irreducible polynomial of the same degree changes the _representation_, but not the underlying thing being represented â€“ as if you still had all the same numbers, but you just changed the name of each one.13.

You can do this for any prime _p_. But in this article weâ€™re concerned with _p_Â =Â 2 in particular. Here are the first few irreducible polynomials over _GF_(2):

_x_  
1 + _x_  
1 + _x_ + _x_<sup>2</sup>  
1 + _x_ + _x_<sup>3</sup>  
1 + _x_<sup>2</sup> + _x_<sup>3</sup>  
1 + _x_ + _x_<sup>4</sup>  
1 + _x_<sup>3</sup> + _x_<sup>4</sup>  
1 + _x_ + _x_<sup>2</sup> + _x_<sup>3</sup> + _x_<sup>4</sup>  
â€¦

Irreducible polynomials over _GF_(2) up to degree 4

A more compact way to write the same information is to use the notation I mentioned earlier, of representing each one as an integer whose pattern of bits gives its coefficients, equivalent to evaluating the polynomial at _x_Â =Â 2. Then the start of the sequence of irreducible polynomials looks like this:

2, 3, 7, 11, 13, 19, 25, 31, 37, 41, 47, 55, 59, 61, 67, 73, 87, 91, 97, 103, 109, 115, 117, 131, 137, 143, 145, 157, 167, 171, 185, 191, 193, 203, 211, 213, 229, 239, 241, 247, 253, 283, 285, 299, 301, 313, 319, 333, 351, 355, 357, 361, 369, 375, 379, 391, 395, 397, 415, 419, 425, 433, 445, 451, 463, 471, 477, 487, 499, 501, 505, â€¦

Read like a sequence of integers<sup id="footnote-oeis">14</sup>Any mathematically interesting sequence of integers has a very good chance of being catalogued in the [On-Line Encyclopedia of Integer Sequences](https://oeis.org/), and this is certainly not an exception. Its catalogue index is [A014580](https://oeis.org/A014580).14, these numbers fascinate me. Theyâ€™re like prime numbers from a parallel universe. If addition were done without carrying, these would _be_ the prime numbers. And, just as youâ€™d hope for something from a parallel universe, it takes you an extra look to spot the difference, because they look very similar to start with â€“ many of the initial ones are _also_ ordinary integer primes. (Though not all â€“ 25 and 55 are the first composite ones; and not all the integer primes appear â€“ 5, 17, and 23 are the first ones missing.)

Thereâ€™s a lot more to say about these larger finite fields. But Iâ€™ll finish up by mentioning what theyâ€™re _useful_ for. They crop up all over cryptography:

-   A finite field of order 2<sup>8</sup> forms a key building block of the standard AES cipher, and also of another cipher (Twofish) which _nearly_ became AES (it was another finalist in the competition to choose a standard block cipher).
-   A finite field of the much larger size 2<sup>128</sup> is used in GCM, which is a fast scheme that combines bulk encryption with integrity protection (known as â€˜AEADâ€™).
-   Finite fields of large power-of-2 size are also sometimes (though not always) used in elliptic-curve cryptography, for both key exchange and digital signatures.
-   Elliptic-curve cryptography is probably going to be abandoned at some point, because of the possibility of working quantum computers being built that can attack it. Finite fields of power-of-2 order _also_ appear in at least one of the replacement â€˜post-quantumâ€™ schemes, as part of the decoding algorithm in the â€œClassic McElieceâ€ key encapsulation system.

These things are big business â€“ and thatâ€™s mostly why CPU architectures bother to implement that polynomial multiplication primitive at all!

With any luck, you should be able to read the footnotes of this article in place, by clicking on the superscript footnote number or the corresponding numbered tab on the right side of the page.

But just in case the CSS didnâ€™t do the right thing, hereâ€™s the text of all the footnotes again:

[1.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-inverse) I apologise for using â€˜invertâ€™ and â€˜inverseâ€™ in two senses in this article. In boolean logic, â€˜invertingâ€™ a signal generally means the NOT operator, interchanging 1 with 0 (or true with false, if you prefer). But in mathematics, an â€˜inverseâ€™ is a value that cancels out another value to get you back to the identity, in a way which varies depending on the operation you use to combine them (if youâ€™re talking about addition itâ€™s âˆ’_x_, and for multiplication itâ€™s 1/_x_). I hope that everywhere Iâ€™ve used the word at all itâ€™s clear from context which sense I mean it in.

[2.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-twos-complement) What if the integers are negative? Normally, in the finite integer sizes that computers handle in hardware, negative integers are represented in twoâ€™s complement, i.e. mod 2<sup><i>n</i></sup>. So âˆ’1 is the integer with all bits 1, for example. Thereâ€™s a reasonably natural way to extend this to _arbitrarily large_ integers, by pretending the string of 1 bits on the left goes all the way to infinity (and it can even be made mathematically rigorous!). In this view, the XOR of two positive numbers, or two negative numbers, is positive, because at a high enough bit position each bit is XORing two 0s or two 1s; but the XOR of a positive and negative number is negative, because sooner or later youâ€™re always XORing a 0 with a 1 bit. Some languages, such as Python, actually implement this â€“ you can try it out at the interactive prompt!

[3.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-emoji) Of course, this doesnâ€™t apply to _all_ Unicode characters! Most donâ€™t have a concept of upper or lower case at all. And unfortunately, this rule isnâ€™t even obeyed by all of the characters that do. It was consistently true in ASCII, and in some of the descendants of ASCII, but Unicode as a whole wasnâ€™t able to stick 100% to the principle. If you take this too far, you might get strange ideas, like the lower-case version of the car emoji being a â€˜no pedestriansâ€™ sign: \>>> chr(ord('ğŸš—') ^ 32)  
'ğŸš·'

[4.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-well-actually) I suppose, _technically_, you could argue that it is still literally true that _a_Â XORÂ _b_, _a_Â +Â _b_, and _a_Â âˆ’Â _b_ are all congruent to each other mod 2, because all that means is that they all have the same low-order bit, which they do. But that isnâ€™t a particularly _useful_ thing to say, because it ignores the way all the higher-order bits do something completely different!

[5.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-integrity) However, encrypting the message is only half the story. Given a good-quality keystream, this technique is good enough to assure _confidentiality_, meaning that an eavesdropper canâ€™t find out what youâ€™re saying. But it doesnâ€™t do one single thing to ensure _integrity_, meaning that if your message is modified by an attacker, the receiver can detect the tampering and know not to trust the modified message. Integrity protection is a completely separate problem. A common mistake in novice cryptosystem design is to leave it out, assuming that if nobody can figure out what the message says, then â€œsurelyâ€ nobody can work out how to tamper with it in a useful way either. Even with more complicated encryption methods than what Iâ€™m describing here, this never goes well!

[6.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-proof-exercise) If youâ€™re mathematically minded and still donâ€™t find this instantly obvious, you might want to try actually _proving_ it. I leave this mostly as an exercise for the reader, but a hint would be: break down each of the input integers _a_ and _b_ as the sum of terms like 2<sup><i>n</i></sup>_a_<sub><i>n</i></sub> (each one being an individual bit of one of the inputs times a power of 2), and collect together the terms on each side that involve the same _n_.

[7.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-or-xor) The extra gate that combines the carry bits can be either an OR gate or an XOR gate, whichever is easier. It doesnâ€™t matter which, because the two types of gate only disagree in the case where both their inputs are 1, and in this circuit, that canâ€™t happen! If the carry from the first half-adder is 1, then its other output _must_ be 0, which means thereâ€™s no carry from the second half-adder.

[8.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-swp) There is an Arm instruction called SWP for â€˜swapâ€™, but itâ€™s not what you want for this kind of purpose. It swaps a register with a value in _memory_, not two registers. Itâ€™s not really for ordinary computation: itâ€™s intended for atomically synchronising between threads, which is very hard to get right and well beyond the scope of this article!

[9.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-aliasing) However, thereâ€™s a subtle â€œgotchaâ€ about this technique. It works fine as long as the two variables youâ€™re swapping are actually different variables, but it fails if they _alias_ each other. For example, if you use this trick to swap two elements of an array, so that `a` and `b` are replaced with `array[i]` and `array[j]`, then what happens if _i_Â =Â _j_? In that situation you probably wanted the â€˜swapâ€™ of an array element with itself to leave it unchanged. But the triple-XOR swap idiom will turn it into zero, which may well _not_ be what you wanted!

[10.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-misere) In fact, Nim comes in two well-known forms, with opposite win conditions. In the variant discussed here, your aim is to take the last counter and leave your opponent with no move. In the other variant, the win condition is exactly the reverse: you aim to make your _opponent_ take the last counter! The strategy for the latter is a tiny bit more complicated, but not too bad. I only discuss the simple version here.

[11.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-infinite-positive-characteristic) However, the converse isnâ€™t true. Any finite field has positive characteristic, but not every field with positive characteristic is finite. There are also _infinite_ fields with this same modular-arithmetic property. But we wonâ€™t get as far as discussing those here.

[12.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-formal-polynomials) You might argue: if _x_ is a number in _GF_(2), then itâ€™s either 0 or 1, and in either case, _x_<sup>2</sup>, or _x_<sup>3</sup>, or any higher power of _x_, are all equal to _x_ itself. So under that assumption, any polynomial of this kind could be simplified by squashing all the higher-order terms down into the _x_<sup>1</sup> term. But we _donâ€™t_ do that here, because we donâ€™t make the assumption that _x_ is one of the two numbers we know about: we leave open the possibility that any two powers _x_<sup><i>i</i></sup> and _x_<sup><i>j</i></sup> might be unequal.

[13.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-unique-finite-fields) In fact, it turns out that _all_ finite fields of a given size _p_<sup><i>d</i></sup> are the same as each other. Choosing a different irreducible polynomial of the same degree changes the _representation_, but not the underlying thing being represented â€“ as if you still had all the same numbers, but you just changed the name of each one.

[14.](https://www.chiark.greenend.org.uk/~sgtatham/quasiblog/xor/#footnote-oeis) Any mathematically interesting sequence of integers has a very good chance of being catalogued in the [On-Line Encyclopedia of Integer Sequences](https://oeis.org/), and this is certainly not an exception. Its catalogue index is [A014580](https://oeis.org/A014580).